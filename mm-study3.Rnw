\documentclass{article}

\newcommand{\gModel}[2]{{$\mathcal{M}^{\circ}(#1, #2)$}}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Raw data acquisition}

<<>>=
data = read.csv("study3-emotions-data.csv")

dataSelf<-data[,c(1,2:19)]
dataSelf1<-data[,c(1,20:37)]
dataSelf2<-data[,c(1,38:55)]
@

Density distribution of the scores:

<<fig=T>>=
dataVector <- na.omit(as.vector(t(dataSelf1[,-1])))
hist(dataVector,probability = T, breaks = "FD")
lines(density(dataVector - 0.5))
@

This is used to simulate random scoring with a similar distribution. This simulation has been performed with the Python script {\tt random-scores-simulator.py} using the following formula for the distribution:

{\tt random() < 0.5 ? 1 : randint(2,7)}.

<<>>=
# 8999 samples gradings, generated by random-scores-simulator.pp
randomdata = read.csv("random-data.csv")

randomDataSelf<-randomdata[,c(1,2:19)]
randomDataSelf1<-randomdata[,c(1,20:37)]
randomDataSelf2<-randomdata[,c(1,38:55)]

# check the density distribution of the random data
#randomdataVector <- na.omit(as.vector(t(randomDataSelf1[,-1])))
#hist(randomdataVector,probability = T, breaks = "FD")
#lines(density(randomdataVector - 0.5))
@

\section{Computation of the degree of accuracy of partners' models}

\gModel{B}{A} and \gModel{C}{A}.

<<>>=
d1<-abs(dataSelf-dataSelf1)
d1$meanDiff<-rowMeans(d1[,c(2:19)],na.rm=T)

d2<-abs(dataSelf-dataSelf2)
d2$meanDiff<-rowMeans(d2[,c(2:19)],na.rm=T)


d<-as.data.frame(cbind(d1$meanDiff,d2$meanDiff))
names(d)<-c("scoreB","scoreC")

# same thing for the random data
randomd1<-abs(randomDataSelf-randomDataSelf1)
randomd1$meanDiff<-rowMeans(randomd1[,c(2:19)],na.rm=T)

randomd2<-abs(randomDataSelf-randomDataSelf2)
randomd2$meanDiff<-rowMeans(randomd2[,c(2:19)],na.rm=T)

randomd<-as.data.frame(cbind(randomd1$meanDiff,randomd2$meanDiff))

@

\section{Computation of $\Delta_1$}

<<>>=

delta1<-c(1:60)
for(i in seq(1, 60, by = 3)) {
  delta1[i]<-abs(d[i,1]-d[i+1,1])
  delta1[i+1]<-abs(d[i+2,1]-d[i,2])
  delta1[i+2]<-abs(d[i+1,2]-d[i+2,2])
}

@

We test $\Delta_1$ against randomly attributed scores (following the same distribution as in the actual data) to check if we observe any symmetrical modelling effects (that would translate into significantly lower $\Delta_1$).

<<fig=T>>=

randomdelta1<-c(1:8999)
for(i in seq(1, 8999, by = 3)) {
  randomdelta1[i]<-abs(randomd[i,1]-randomd[i+1,1])
  randomdelta1[i+1]<-abs(randomd[i+2,1]-randomd[i,2])
  randomdelta1[i+2]<-abs(randomd[i+1,2]-randomd[i+2,2])
}


boxplot(cbind(delta1, randomdelta1))

# are variances equal?
var.test(as.vector(delta1), as.vector(randomdelta1))

t.test(delta1, randomdelta1,var.equal = F)
@

The symmetry hypothesis tells that \emph{good} modelling should be symmetrical, but does not say much about \emph{bad} modelling.

hence, the effect should be even stronger if we check the average $\Delta_1$ of the best modellers against random data.

To do so, we first compute the average grading score per triad.

<<fig=T, echo=F>>=
sub<-c(1:60)
d$gID<-1+as.integer(sub/3 - 0.1)

d$meanDiff<-(d$scoreB+d$scoreC)/2
t1<-tapply(d$meanDiff,d$gID,mean)
d$meanTeam<-rep(t1,each=3)

hist(d$meanTeam, breaks = 20)
@

A dip test confirms that the distribution is not uni-modal:

<<>>=
diptest::dip.test(d$meanTeam)
@

We use a K-Mean clustering to know where to cut our group of modellers, provinding an initial estimate of the clusters' centers:

<<>>=
centers <- c(1:2)
centers[1] = 0.8
centers[2] = 1.2
clusters<-kmeans(d$meanTeam,centers)

# alternatively, let kmeans find the centers. But this may change between runs due to the initial random initialisation of the algo
#clusters<-kmeans(d$meanTeam,2)

clusters$centers
top <- max(d$meanTeam[clusters$cluster == 1])
bottom <- min(d$meanTeam[clusters$cluster == 2])
d$teamRating<-ifelse(d$meanTeam<=(top+bottom)/2,"good","bad")
@

We finally re-run a t-test on this subset of the modellers:

<<fig=T>>=
boxplot(cbind(delta1[d$teamRating=="good"], randomdelta1))
t.test(delta1[d$teamRating=="good"], randomdelta1,var.equal = F)
@

This confirms the symmetry hypothesis: the difference of scores for good modellers with random data is even stronger.

\section{Computation of $\Delta_2$}

<<>>=
delta2<-c(1:60)
for(i in seq(1, 60, by = 3)) {
  delta2[i]<-abs(d[i,1]-d[i,2])
  delta2[i+1]<-abs(d[i+1,1]-d[i+1,2])
  delta2[i+2]<-abs(d[i+2,1]-d[i+2,2])
}
@

As previously, we test $\Delta_2$ against randomly attributed scores to check if we observe any similar modelling behaviours between two persons judging the same third one (which would translate into significantly lower $\Delta_2$ compared to chance):

<<echo=F>>=
randomdelta2<-c(1:8999)
for(i in seq(1, 8999, by = 3)) {
  randomdelta2[i]<-abs(randomd[i,1]-randomd[i,2])
  randomdelta2[i+1]<-abs(randomd[i+1,1]-randomd[i+1,2])
  randomdelta2[i+2]<-abs(randomd[i+2,1]-randomd[i+2,2])
}
@

<<fig=T>>=
boxplot(cbind(delta2, randomdelta2))
t.test(delta2, randomdelta2)
@

In average, $\Delta_2$ is not significantly smaller than chance: this does not support the hypothesis of \gModel{A}{C} and \gModel{B}{C} being correlated, ie modelling accuracy would either comes for the overall quality of the interaction, or C fosters good models by making his emotional state easily readable.

\section{Computation of $\Delta_3$}

<<>>=
delta3<-c(1:60)
for(i in seq(1, 60, by = 3)) {
  delta3[i]<-abs(d[i+1,1]-d[i+2,1])
  delta3[i+1]<-abs(d[i,1]-d[i+2,2])
  delta3[i+2]<-abs(d[i,2]-d[i+1,2])
}
@

As previously, we test $\Delta_3$ against randomly attributed scores to check if we observe any similar modelling behaviours between two persons judging the same third one (which would translate into significantly lower $\Delta_3$ compared to chance):

<<echo=F>>=
randomdelta3<-c(1:8999)
for(i in seq(1, 8999, by = 3)) {
  randomdelta3[i]<-abs(randomd[i+1,1]-randomd[i+2,1])
  randomdelta3[i+1]<-abs(randomd[i,1]-randomd[i+2,2])
  randomdelta3[i+2]<-abs(randomd[i,2]-randomd[i+1,2])
}
@

<<fig=T>>=
boxplot(cbind(delta3, randomdelta3))
t.test(delta3, randomdelta3)
@

We obtain a significantly lower $\Delta_3$, which supports the hypothesis of \gModel{C}{A} and \gModel{C}{B} being correlated: it supports the hypothesis that some persons are better modellers than others.

\end{document}